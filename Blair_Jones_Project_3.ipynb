{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Notebook\n",
    "### Class: W205.5 - Data Engineering\n",
    "    Student: Blair Jones\n",
    "\n",
    "    Date: 23 Oct 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "This project is an end-to-end simulation of a event streaming analytics solution.  A fictional game, Placebo, generates events as players interact with the system.  The system processes events as they are received and prepares them for analysis by data scientists working for the game company.\n",
    "\n",
    "## Objective / Problem Statement\n",
    "- Create an event processing pipeline to receive event data from game clients\n",
    "- Transform game events into a form suitable for usage by data scientists\n",
    "- Save event data into storage that will be accessible to the data scientists\n",
    "- Demonstrate sample business queries\n",
    "\n",
    "### Sample business queries include\n",
    "\n",
    "#### For Game Designers\n",
    "\n",
    "* Descriptive: What is the mean time between when a player logs in for the first time and when they purchase an item (weapon, spell, etc.)?\n",
    "* Descriptive: What is the mean time between when a player logs in for the first time and they join an org (guild, team, etc.)?\t\n",
    "* Explanatory Analysis:  Which tactics (special offers, lower prices, social shares, etc.) are most effective at reducing the mean time for a player to purchase an item?\n",
    "* Predictive:  How likely is it that the average player will be able to defeat a new AI-characters?  (ie. is the AI too smart, too tough?)\n",
    "\n",
    "#### For Player Dashboard\n",
    "\n",
    "* Descriptive Analysis:  What is the distribution of player wealth? (ie how many players have what amount of wealth?)\n",
    "* Descriptive Analysis:  What is the distribution of player wealth by time spent playing the game? (ie do players just accumulate wealth or do they spend it?)\n",
    "* Explanatory Analysis:  What are the highest contributing factors (ie. character attributes, length of play, # guilds joined, # items in inventory)?\n",
    "* Predictive Analysis:  What amount of wealth is a player likely to accumulate as they play the game?\n",
    "\n",
    "\n",
    "## Solution Approach\n",
    "- The architecture is intended to receive data from game clients (apps, web pages, consoles), and transform and publish it in a way that data scientists can analyze the information.\n",
    "- The data may also be used to create dashboards for players.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No specific Python libraries were used for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical component view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: gray'>\n",
    "    <img src='./images/pipeline-overall.svg' />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology Choices (for in-scope components)\n",
    "\n",
    "#### Driving principles\n",
    "The solution requires low-latency, scalability and portability.  It must also be flexibility to adapt as new functional requirements are identified.\n",
    "\n",
    "\n",
    "#### Technologies\n",
    "- Data Sources (event logs)\n",
    "    - Game client is not in scope.\n",
    "    - A Game API will be created and used to generate synthetic business events.  Flask will be used to create the API.\n",
    "    \n",
    "    \n",
    "- Streaming Context\n",
    "    - Topic Queues: Kafka\n",
    "    - Data Transformation: Spark\n",
    "    \n",
    "    \n",
    "- Distributed Analytics Storage\n",
    "    - Files:  Hadoop (HDFS)\n",
    "\n",
    "\n",
    "- Player Session Data\n",
    "    - Key-value store: Redis\n",
    "\n",
    "\n",
    "- Runtime Platform\n",
    "    - Containers: Docker, Docker-Compose.  All other technologies will be run inside Docker containers.  The use of Docker ensure portability to different hosting options.\n",
    "    - Hosting: Google Cloud Platform (GCP).  All components will be deployed to GCP for development and testing.  The choice of production environment can be made later.  \n",
    "\n",
    "\n",
    "- Query Tools\n",
    "    - Presto\n",
    "    \n",
    "    \n",
    "- Test Data Generation\n",
    "    - curl (for unit test cases)\n",
    "    - Apache Bench (for bulk data generation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Design\n",
    "\n",
    "### Game Events\n",
    "\n",
    "The Game API module receives events as routed urls that contain arguments.\n",
    "\n",
    "* GET methods are used by some routes to return the requested information, for example to display a user profile.\n",
    "\n",
    "* POST methods are used by some routes to receive parameters for specific actions, such as \"buy a sword\".  In this case the arguments will specify the type of sword and its cost.\n",
    "\n",
    "Game events implemented in this project (arguments shown in parentheses):\n",
    "- Register player (name, password, wealth:optional)\n",
    "- Login (username, password)\n",
    "- Buy a sword (type, cost)\n",
    "- Join a guild (org, level, cost)\n",
    "- Buy a spell (type, cost)\n",
    "- Join a team (org, level)\n",
    "- View user profile (username)\n",
    "\n",
    "Events are implemented with Flask as routes.  Each route generates a unique ID for the event, stores key metadata associated with the event, extracts expected parameters from the http request, applies relevant business logic, writes an event out to a Kafka topic, then returns a suitable response to the invoking client.\n",
    "\n",
    "For the purpose of this project, the simulated response to the client is either a simple html-formatted response or a simple text message.  In future versiona a more formal json data structure is appropriate to consider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player Session\n",
    "\n",
    "A player's session data is stored in Redis while the platform is running.  The basic data structure is a single value for each player's state saved under the key of the player's id.  The system is intended to generate a unique ID for each player.\n",
    "\n",
    "Specific player attributes implemented for the purpose of this project:\n",
    "- ID (string of UUID)\n",
    "- Name (string)\n",
    "- Password (string encrypted)\n",
    "- Wealth (int)\n",
    "- Health (int)\n",
    "- Date created (string of timestamp)\n",
    "- Inventory (list of strings)\n",
    "- Affiliations (list of dictionary items)\n",
    "\n",
    "Inventory represents a list containing individual items in the player's inventory.  Each item is a string specifying the item.  Future versions could implement this as a list of json objects where each object contains additional data about the item, such as number of same type of items, availability, etc.\n",
    "\n",
    "Affiliations represents a list containing individual organizations a player belongs to.  Each organization is a dictionary specifying the name of the organization, the level of the player, and the timestamp when joined.  An organization can be a guild, or a team created amongst players.\n",
    "\n",
    "For the purpose of this project, the fictional game client software is responsible for consulting the latest version of the player's state (health, wealth, inventory, etc.).  For this reason, this simulation does not return modified data to the client in the API response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Topics \n",
    "\n",
    "The Topic queues are the front-end for the event streaming and analytics platform.  They receive relevant business events from the game client/API.  Transformation logic is applied to wrangle the data into a form suitable for use by data scientists.\n",
    "\n",
    "The inbound Topics for this project are:\n",
    "\n",
    "* all_events: Every game event, regardless of type, is written to all_events.  This can then be filtered for any type of event or multiple types of events to analyze player behaviors.\n",
    "* buy_events: Related to all commercial events.\n",
    "* social_events: Related to joining or leaving guilds or teams, advancing in level, attaining achievements, etc.   Intended to use for specialized machine learning.\n",
    "\n",
    "\n",
    "Outbound Topics identified are:\n",
    "\n",
    "* player_notifications: Messages intended to be displayed to players at next-login or during game play.  Can also be used for analytics.  This topic is not implemented in this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "The data extraction and transformation logic is relatively simple for this project due to the fairly streamlined definition of objects in the game API.\n",
    "\n",
    "The key requirement is to flatten objects that have nested structures, which may not have consistent structures in the nested objects, so that they can be stored in dataframes for easy use in queries.\n",
    "\n",
    "For example, the Player Session object contains lists for both Inventory and Affiliations.  Each of these lists may contain none, one or multiple items.  Each item may be a simple string or a json object.  To simplify queries, these structures need to be flattened in PySpark for storage into Hadoop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage\n",
    "\n",
    "Storage in Hadoop is simplified after the transformations achieved with Spark.  We simply save the dataframe generated through the transformations into an appropriately named Hadoop fileset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries\n",
    "\n",
    "Queries are built in Presto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0) Environment Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Game API Setup\n",
    "\n",
    "**Instance Setup**\n",
    "\n",
    "Redis is used in the Game API for managing persistent player session data.  We must install Redis before launching the Game API.  In the future this should be added to the MIDS image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/w205/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/w205/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied (use --upgrade to upgrade): redis in /usr/local/lib/python2.7/dist-packages\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids pip install redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API Launch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose exec mids env FLASK_APP=/w205/project-3-bjonesneu/game_api.py flask run --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Kafka Topic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\n",
      "WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\n",
      "WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec kafka kafka-topics --create --topic placebo_all_events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "!docker-compose exec kafka kafka-topics --create --topic placebo_buy_events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "!docker-compose exec kafka kafka-topics --create --topic placebo_social_events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: placebo_all_events\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
      "\tTopic: placebo_all_events\tPartition: 0\tLeader: 1\tReplicas: 1\tIsr: 1\n",
      "Topic: placebo_buy_events\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
      "\tTopic: placebo_buy_events\tPartition: 0\tLeader: 1\tReplicas: 1\tIsr: 1\n",
      "Topic: placebo_social_events\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
      "\tTopic: placebo_social_events\tPartition: 0\tLeader: 1\tReplicas: 1\tIsr: 1\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec kafka kafka-topics --describe --topic placebo_all_events --zookeeper zookeeper:32181\n",
    "!docker-compose exec kafka kafka-topics --describe --topic placebo_buy_events --zookeeper zookeeper:32181\n",
    "!docker-compose exec kafka kafka-topics --describe --topic placebo_social_events --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Test Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple calls to Game API to create events in Kafka Topics for development and testing.  These statements were executed multiple times to generate a full set of test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n",
      "<title>Redirecting...</title>\n",
      "<h1>Redirecting...</h1>\n",
      "<p>You should be redirected automatically to target URL: <a href=\"static/register.html\">static/register.html</a>.  If not click the link.Created user +:)\n",
      "Sword purchased [:)\n",
      "You do not have sufficient funds [:(\n",
      "Joined #guild\n",
      "Created user +:)\n",
      "Spell purchased <:)\n",
      "Spell purchased <:)\n",
      "Joined #guild\n",
      "User data: \n",
      "\n",
      "{\"user_id\": \"987611\", \"wealth\": -2654.0, \"created\": \"2020-11-23 14:58:44.356273\", \"user_pwd\": \"****\", \"affiliations\": [{\"joined_ts\": \"2020-11-23 14:58:44.738252\", \"org\": \"soldiers\", \"level\": \"masterofarms\"}], \"health\": 100, \"inventory\": [\"cutlass\"], \"user_name\": \"dirk\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:5000/\n",
    "!curl -d 'userid=987611&userpwd=blahdh&username=dirk&wealth=12345' http://localhost:5000/create/user\n",
    "!curl -d 'userid=987611&type=cutlass&cost=5000' http://localhost:5000/buy/sword\n",
    "!curl -d 'userid=987611&type=longsword&cost=10000' http://localhost:5000/buy/sword\n",
    "!curl -d 'userid=987611&org=soldiers&level=masterofarms&cost=9999' http://localhost:5000/join/guild\n",
    "!curl -d 'userid=1234&userpwd=yahbadaba&username=merly&wealth=98712' http://localhost:5000/create/user\n",
    "!curl -d 'userid=1234&type=charm&cost=4999' http://localhost:5000/buy/spell\n",
    "!curl -d 'userid=1234&type=charm&cost=4999' http://localhost:5000/buy/spell\n",
    "!curl -d 'userid=1234&org=magicians&level=accolyte&cost=99' http://localhost:5000/join/guild\n",
    "!curl http://localhost:5000/data/user?userid=987611"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify events created in Kafka Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t placebo_all_events -o beginning -e\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Hadoop Storage Setup\n",
    "\n",
    "**Verify Status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "drwxr-xr-x   - root   supergroup          0 2020-11-23 14:36 /tmp/buys\n",
      "drwxrwxrwt   - root   supergroup          0 2020-11-22 21:33 /tmp/checkpoints_for_buys\n",
      "drwxrwxrwt   - root   supergroup          0 2020-11-22 21:33 /tmp/checkpoints_for_social\n",
      "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - root   supergroup          0 2020-11-22 21:33 /tmp/hive\n",
      "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n",
      "drwxr-xr-x   - root   supergroup          0 2020-11-23 14:36 /tmp/social\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Spark Stream Setup\n",
    "\n",
    "This section is executed using the CLI.  The code used is copied here for reference.\n",
    "\n",
    "In a dedicated terminal session:\n",
    "```code\n",
    "CLI:   docker-compose exec spark spark-submit /w205/project-3-bjonesneu/process_buy_stream.py\n",
    "```\n",
    "\n",
    "In a dedicated terminal session:\n",
    "```code\n",
    "CLI:   docker-compose exec spark spark-submit /w205/project-3-bjonesneu/process_social_stream.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Presto Query Setup\n",
    "\n",
    "#### Setup Hive table(s)\n",
    "\n",
    "6a) Launch Hive in a dedicated terminal session:\n",
    "```code\n",
    "CLI:  docker-compose exec cloudera hive\n",
    "```\n",
    "\n",
    "6b) Within the Hive console, run:\n",
    "```code\n",
    "create external table if not exists default.buys ( \n",
    "    raw_event string,\n",
    "    spk_ts string,\n",
    "    accept string,\n",
    "    host string,\n",
    "    user_agent string,\n",
    "    timestamp string,\n",
    "    event_type string,\n",
    "    user_id string,\n",
    "    status string,\n",
    "    type string,\n",
    "    cost double,\n",
    "    player_created string,\n",
    "    player_wealth double,\n",
    "    player_num_items int\n",
    ")\n",
    "stored as parquet\n",
    "location '/tmp/buys'\n",
    "tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "\n",
    "create external table if not exists default.social ( \n",
    "    raw_event string,\n",
    "    spk_ts string,\n",
    "    accept string,\n",
    "    host string,\n",
    "    user_agent string,\n",
    "    timestamp string,\n",
    "    event_type string,\n",
    "    user_id string,\n",
    "    status string,\n",
    "    org string,\n",
    "    level string,\n",
    "    cost double,\n",
    "    player_created string,\n",
    "    player_wealth double,\n",
    "    player_num_affiliations int\n",
    "\n",
    ")\n",
    "stored as parquet\n",
    "location '/tmp/social'\n",
    "tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "\n",
    "6c) Launch Presto in a dedicated terminal session:\n",
    "```code\n",
    "CLI:  docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "```\n",
    "\n",
    "6d) Run queries within the Presto console:\n",
    "```code\n",
    "show tables;\n",
    "```\n",
    "        Output:\n",
    "             Table  \n",
    "            --------\n",
    "             buys   \n",
    "             social \n",
    "            (2 rows)\n",
    "\n",
    "```code\n",
    "describe buys;\n",
    "```\n",
    "        Output:\n",
    "                  Column      |  Type   | Comment \n",
    "            ------------------+---------+---------\n",
    "             raw_event        | varchar |         \n",
    "             spk_ts           | varchar |         \n",
    "             accept           | varchar |         \n",
    "             host             | varchar |         \n",
    "             user_agent       | varchar |         \n",
    "             timestamp        | varchar |         \n",
    "             event_type       | varchar |         \n",
    "             user_id          | varchar |         \n",
    "             status           | varchar |         \n",
    "             type             | varchar |         \n",
    "             cost             | double  |         \n",
    "             player_created   | varchar |         \n",
    "             player_wealth    | double  |         \n",
    "             player_num_items | integer |         \n",
    "            (14 rows)\n",
    "\n",
    "```code\n",
    "select timestamp, status, type, cost from buys;\n",
    "```\n",
    "    Output:\n",
    "                 timestamp          |      status       |  type   | cost  \n",
    "        ----------------------------+-------------------+---------+-------\n",
    "         2020-11-22 22:00:15.706369 | approved_purchase | saber   | 999.0 \n",
    "         2020-11-22 22:00:15.712180 | approved_purchase | saber   | 999.0 \n",
    "         2020-11-22 22:00:15.725177 | approved_purchase | saber   | 999.0 \n",
    "         2020-11-22 22:00:15.729610 | approved_purchase | saber   | 999.0 \n",
    "         2020-11-22 22:00:15.735913 | approved_purchase | saber   | 999.0 \n",
    "         2020-11-22 22:00:15.739464 | approved_purchase | saber   | 999.0 \n",
    "         2020-11-22 22:00:15.743363 | approved_purchase | saber   | 999.0 \n",
    "         2020-11-22 22:00:15.753620 | approved_purchase | saber   | 999.0 \n",
    "         2020-11-22 22:00:16.357225 | approved_purchase | potion  |  50.0 \n",
    "         2020-11-22 22:00:16.362849 | approved_purchase | potion  |  50.0 \n",
    "         2020-11-22 22:00:16.374195 | approved_purchase | potion  |  50.0 \n",
    "         2020-11-22 22:00:16.381072 | approved_purchase | potion  |  50.0 \n",
    "         2020-11-22 22:00:16.388540 | approved_purchase | potion  |  50.0 \n",
    "         2020-11-22 22:00:16.398242 | approved_purchase | potion  |  50.0 \n",
    "         2020-11-22 22:00:16.407975 | approved_purchase | potion  |  50.0 \n",
    "         2020-11-22 22:00:17.001598 | approved_purchase | cutlass | 777.0 \n",
    "         2020-11-22 22:00:17.010693 | approved_purchase | cutlass | 777.0 \n",
    "         2020-11-22 22:00:17.017223 | approved_purchase | cutlass | 777.0 \n",
    "         2020-11-22 22:00:17.026247 | approved_purchase | cutlass | 777.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup base data for stream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -d 'userid=987611&userpwd=blahdh&username=dirk&wealth=1234500' http://localhost:5000/create/user\n",
    "!curl -d 'userid=123456&userpwd=yahbadaba&username=merly&wealth=9871200' http://localhost:5000/create/user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate continuous event stream**\n",
    "\n",
    "In a dedicated terminal session:\n",
    "```code\n",
    "while true; do\n",
    "\n",
    "  docker-compose exec mids ab -n $(($RANDOM%10 )) \\\n",
    "  -p /w205/project-3-bjonesneu/test/post-buy$(($RANDOM%5 )).txt \\\n",
    "  -T \"application/json\" -H \"Host: user1.comcast.com\" http://localhost:5000/buy/spell\n",
    "  \n",
    "  docker-compose exec mids ab -n $(($RANDOM%10 )) \\\n",
    "  -p /w205/project-3-bjonesneu/test/post-buy$(($RANDOM%5 )).txt \\\n",
    "  -T \"application/json\" -H \"Host: user1.comcast.com\" http://localhost:5000/buy/sword\n",
    "\n",
    "  docker-compose exec mids ab -n $(($RANDOM%10 )) \\\n",
    "  -p /w205/project-3-bjonesneu/test/post-buy$(($RANDOM%5 )).txt \\\n",
    "  -T \"application/json\" -H \"Host: user1.comcast.com\" http://localhost:5000/buy/spell\n",
    "\n",
    "  docker-compose exec mids ab -n $(($RANDOM%10 )) \\\n",
    "  -p /w205/project-3-bjonesneu/test/post-join$(($RANDOM%5 )).txt \\\n",
    "  -T \"application/json\" -H \"Host: user1.comcast.com\" http://localhost:5000/join/guild\n",
    "\n",
    "  docker-compose exec mids ab -n $(($RANDOM%10 )) \\\n",
    "  -p /w205/project-3-bjonesneu/test/post-join$(($RANDOM%5 )).txt \\\n",
    "  -T \"application/json\" -H \"Host: user1.comcast.com\" http://localhost:5000/join/team\n",
    "\n",
    "sleep 5\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Queries\n",
    "\n",
    "Here are sample queries that may be helpful for future data science team users of this platform.\n",
    "\n",
    "NOTE:  All these queries are executed within the dedicated Presto session.\n",
    "\n",
    "\n",
    "### Query - How many items of each type were purchased?\n",
    "\n",
    "```sql\n",
    "select status, type, count(type) as num_items from buys group by status, type order by status;\n",
    "```\n",
    "        Output:\n",
    "                   status       |   type    | num_items \n",
    "            --------------------+-----------+----------\n",
    "             approved_purchase  | saber     |   106 \n",
    "             approved_purchase  | potion    |    44 \n",
    "             approved_purchase  | dagger    |    36 \n",
    "             approved_purchase  | cutlass   |    39 \n",
    "             approved_purchase  | charm     |     2 \n",
    "             insufficient_funds | longsword |     1 \n",
    "            (6 rows)\n",
    "\n",
    "\n",
    "### Query - What is the average price paid for each type of item?\n",
    "\n",
    "```sql\n",
    "select type, round(avg(cost), 2) as avg_cost from buys where status='approved_purchase' group by type order by type;\n",
    "```\n",
    "        Output:\n",
    "                type    | avg_cost \n",
    "            ------------+----------\n",
    "             broadsword |   1034.0 \n",
    "             charm      |   4999.0 \n",
    "             cutlass    |    777.0 \n",
    "             dagger     |     55.0 \n",
    "             potion     |     50.0 \n",
    "             saber      |    999.0 \n",
    "            (6 rows)\n",
    "\n",
    "### Query - Which team or guild is most popular?\n",
    "\n",
    "```sql\n",
    "select event_type, org, count(org) as num_joins from social group by event_type, org order by num_joins desc; \n",
    "```\n",
    "        Output:\n",
    "             event_type |      org       | num_joins \n",
    "            ------------+----------------+-----------\n",
    "             join_team  | Magician Guild |       121 \n",
    "             join_team  | team ABCD      |       100 \n",
    "             join_guild | Magician Guild |        94 \n",
    "             join_guild | Golds Gym      |        85 \n",
    "             join_guild | team BLAH      |        83 \n",
    "             join_guild | santas shop    |        80 \n",
    "             join_team  | Golds Gym      |        80 \n",
    "             join_team  | santas shop    |        77 \n",
    "             join_guild | team ABCD      |        70 \n",
    "             join_team  | team BLAH      |        61 \n",
    "            (10 rows)\n",
    "\n",
    "### Query - What is the average wealth of players joining each organization?\n",
    "\n",
    "```sql\n",
    "select org, round(avg(player_wealth), 2) as avg_wealth from social group by org order by org; \n",
    "```\n",
    "        Output:\n",
    "                  org       | avg_wealth \n",
    "            ----------------+------------\n",
    "             Golds Gym      | 1202045.52 \n",
    "             Magician Guild | 9617308.02 \n",
    "             santas shop    | 1204290.71 \n",
    "             team ABCD      | 9615629.38 \n",
    "             team BLAH      | 9521707.42 \n",
    "            (5 rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Features\n",
    "\n",
    "A number of optional features were implemented in this project:\n",
    "\n",
    "- Several types of events were generated from the Game API and filtered in the pyspark stream.  These events had different API signatures as well as generated different data structures.\n",
    "\n",
    "- The API used two http verbs for different purposes:\n",
    "    * `GET` for data-request style functions (lookup user profile)\n",
    "    * `POST` for data-update style functions (buy an item, join an org)\n",
    "\n",
    "\n",
    "- The API accepted different *parameters* for each function.\n",
    "\n",
    "- The Redis database was used by the Game API to track player state during gameplay.  This maintained the player's inventory of items, wealth after purchases, and list of affiliations.  Other attributes were configured but not used for this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Thoughts\n",
    "\n",
    "This project demonstrates the implementation of an end-to-end event streaming and analytics pipeline using a number of open-source technologies:\n",
    "* Simulated clients invoke an API\n",
    "* The API generates events into queues\n",
    "* Events are incrementally read from queues, filtered and written to relevant event datastores\n",
    "* Queries are used to analyze the data streaming into the event datastores\n",
    "\n",
    "This is a simple demonstration of a configurable, resilient and scalable platform typically used for streaming analytics work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
